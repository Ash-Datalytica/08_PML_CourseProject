#return Factor and outcome columns into the testing set
testingPC <- cbind(testingPC,
testing[,setdiff(1:dim(testing)[2], union(numColumns, notUsedColumns))])
nPredictors <- dim (trainingPC)[2]-1
#Using repeated CV, http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm
system.time ({
# modFit <- train (classe ~ . ,
#                  data=training[,-notUsedColumns], method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                  )
modFit <- train (classe ~ . ,
data=trainingPC, method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
}) #5% - 76 sec, 10% - 237sec
#35 predictors:
#5% - 30 sec, acc=0.7
#tuneGrid=data.frame(mtry=2)
modFit
#check Accuracy
confusionMatrix(predict (modFit, newdata = testingPC), testingPC$classe)
#Accuracy= 5%-94%, 10% - 98%
nPredictors <- dim (trainingPC)[2]-1
system.time ({
# modFit <- train (classe ~ . ,
#                  data=training[,-notUsedColumns], method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                  )
modFit <- train (classe ~ . ,
data=trainingPC, method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
}) #5% - 76 sec, 10% - 237sec
modFit
confusionMatrix(predict (modFit, newdata = testingPC), testingPC$classe)
set.seed (1234)
inTrain <- createDataPartition(y = train$classe, p=0.25, list=FALSE)
training <- train [inTrain,]
testing <- train [-inTrain,]
#don't touch test data
# str(test)
# View(test)
notUsedColumns <- c(1,5) #id, dates
#won't use columns where all values are NA
notUsedColumns <- union(notUsedColumns,
which(apply(training, 2, function(x) sum(is.na(x))) == dim (training)[1]))
#Exclude near zero variance variables
# notUsedColumns <- union(notUsedColumns,
#                 which(names (training) %in% c("amplitude_yaw_belt",
#                              "amplitude_yaw_dumbbell", "amplitude_yaw_forearm")))
nsv <- nearZeroVar (training, saveMetrics=TRUE)
notUsedColumns <- union(notUsedColumns, which(nsv$zeroVar))
#notUsedColumns
#convert all used character columns into numeric
charColumns <- which(sapply(training, class) == "character")
charColumns <- setdiff(charColumns, notUsedColumns )
training[,charColumns] <- apply(training[, charColumns], 2, as.numeric)
#impute NA's for integer and numeric (not Factor or character)
numColumns <- setdiff(which(sapply(training, class) %in% c("numeric", "integer")), notUsedColumns)
#sum (apply(training[,numColumns], 2, class) != "numeric") #=0 => all numeric
#impute NA's
preObj <- preProcess(training[,numColumns], method="knnImpute") #K-nearest neighbours imputation
training[,numColumns] <- predict(preObj, training[,numColumns])
#try to reduce number of features by using PCA
preProc <- preProcess(training[,numColumns], method="pca", thresh=0.95)
preProc #33 columns
trainingPC <- predict(preProc, training[,numColumns])
#return Factor and outcome columns into the training set
trainingPC <- cbind(trainingPC,
training[,setdiff(1:dim(training)[2], union(numColumns, notUsedColumns))])
#process testing features
testing[,charColumns] <- apply(testing[, charColumns], 2, as.numeric)
#impute NA's in testing
testing[,numColumns] <- predict(preObj, testing[,numColumns])
testingPC <- predict (preProc, testing[,numColumns]) #get PC for testing based on the training preprocessing
#return Factor and outcome columns into the testing set
testingPC <- cbind(testingPC,
testing[,setdiff(1:dim(testing)[2], union(numColumns, notUsedColumns))])
notUsedColumns <- c(1,5) #id, dates
notUsedColumns <- union(notUsedColumns,
which(apply(training, 2, function(x) sum(is.na(x))) == dim (training)[1]))
notUsedColumns
nsv <- nearZeroVar (training, saveMetrics=TRUE)
which(nsv$zeroVar)
notUsedColumns <- c(1,5) #id, dates
nsv <- nearZeroVar (training, saveMetrics=TRUE)
notUsedColumns <- union(notUsedColumns, which(nsv$zeroVar))
notUsedColumns
charColumns <- which(sapply(training, class) == "character")
charColumns <- setdiff(charColumns, notUsedColumns )
training[,charColumns] <- apply(training[, charColumns], 2, as.numeric)
numColumns <- setdiff(which(sapply(training, class) %in% c("numeric", "integer")), notUsedColumns)
preObj <- preProcess(training[,numColumns], method="knnImpute") #K-nearest neighbours imputation
training[,numColumns] <- predict(preObj, training[,numColumns])
preProc <- preProcess(training[,numColumns], method="pca", thresh=0.95)
preProc #33 columns
trainingPC <- predict(preProc, training[,numColumns])
trainingPC <- cbind(trainingPC,
training[,setdiff(1:dim(training)[2], union(numColumns, notUsedColumns))])
testing[,charColumns] <- apply(testing[, charColumns], 2, as.numeric)
testing[,numColumns] <- predict(preObj, testing[,numColumns])
testingPC <- predict (preProc, testing[,numColumns]) #get PC for testing based on the training preprocessing
testingPC <- cbind(testingPC,
testing[,setdiff(1:dim(testing)[2], union(numColumns, notUsedColumns))])
nPredictors <- dim (trainingPC)[2]-1
system.time ({
# modFit <- train (classe ~ . ,
#                  data=training[,-notUsedColumns], method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                  )
modFit <- train (classe ~ . ,
data=trainingPC, method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
}) #5% - 76 sec, 10% - 237sec
#35 predictors:
#5% - 30 sec, acc=0.7
#tuneGrid=data.frame(mtry=2)
modFit
system.time ({
# modFit <- train (classe ~ . ,
#                  data=training[,-notUsedColumns], method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                  )
modFit <- train (classe ~ . ,
data=trainingPC, method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
}) #5% - 76 sec, 10% - 237sec
#35 predictors:
#5% - 30 sec, acc=0.7
#tuneGrid=data.frame(mtry=2)
modFit
system.time ({
# modFit <- train (classe ~ . ,
#                  data=training[,-notUsedColumns], method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                  )
modFit <- train (classe ~ . ,
data=trainingPC, method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
}) #5% - 76 sec, 10% - 237sec
stopCluster(cl) # Explicitly free up cores again.
cl<-makeCluster(5) # Assign number of cores you want to use; in this case use 5 cores
registerDoSNOW(cl) # Register the cores.
nPredictors <- dim (trainingPC)[2]-1
#Using repeated CV, http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm
system.time ({
# modFit <- train (classe ~ . ,
#                  data=training[,-notUsedColumns], method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                  )
modFit <- train (classe ~ . ,
data=trainingPC, method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
}) #5% - 76 sec, 10% - 237sec
nPredictors <- dim (trainingPC)[2]-1
#Using repeated CV, http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm
system.time ({
# modFit <- train (classe ~ . ,
#                  data=training[,-notUsedColumns], method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                  )
modFit <- train (classe ~ . ,
data=trainingPC, method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
}) #5% - 76 sec, 10% - 237sec
library("doSNOW")
library (caret)
set.seed (1234)
inTrain <- createDataPartition(y = train$classe, p=0.7, list=FALSE)
training <- train [inTrain,]
testing <- train [-inTrain,]
#don't touch test data
# str(test)
# View(test)
#str(training)
#View(training)
notUsedColumns <- union(notUsedColumns,
nearZeroVar (training, saveMetrics=FALSE, uniqueCut = 0.05))
#notUsedColumns
#impute NA's for integer and numeric (not Factor or character)
numColumns <- setdiff(which(sapply(training, class) %in% c("numeric", "integer")), notUsedColumns)
preObj <- preProcess(training[,numColumns], method="knnImpute") #K-nearest neighbours imputation
training[,numColumns] <- predict(preObj, training[,numColumns])
#try to reduce number of features by using PCA
preProc <- preProcess(training[,numColumns], method="pca", thresh=0.999)
preProc #35 columns (25%)
trainingPC <- predict(preProc, training[,numColumns])
#return Factor and outcome columns into the training set
trainingPC <- cbind(trainingPC,
training[,c("user_name", "new_window", "classe")])
#View(trainingPC)
## Process testing features
#impute NA's in testing
testing[,numColumns] <- predict(preObj, testing[,numColumns])
testingPC <- predict (preProc, testing[,numColumns]) #get PC for testing based on the training preprocessing
#return Factor and outcome columns into the testing set
testingPC <- cbind(testingPC,
testing[,c("user_name", "new_window", "classe")])
cl<-makeCluster(4) # Assign number of cores you want to use; in this case use 4 cores
registerDoSNOW(cl) # Register the cores.
#try SVM
## try RF
nPredictors <- dim (trainingPC)[2]-1
system.time ({
# modFit <- train (classe ~ . ,
#                  data=training[,-notUsedColumns], method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                  )
modFit <- train (classe ~ . ,
data=trainingPC, method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
}) #5% - 76 sec, 10% - 237sec
stopCluster(cl) # Explicitly free up cores again.
cl<-makeCluster(2)
registerDoSNOW(cl) # Register the cores.
nPredictors <- dim (trainingPC)[2]-1
#Using repeated CV, http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm
system.time ({
# modFit <- train (classe ~ . ,
#                  data=training[,-notUsedColumns], method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                  )
modFit <- train (classe ~ . ,
data=trainingPC, method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
}) #5% - 76 sec, 10% - 237sec
#35 predictors:
#5% - 30 sec, acc=0.7
#37 predictors, 25% - 500sec, acc=.89
#44 predictors:
#25% - 30 sec, acc=0.7
#89 predictor
#tuneGrid=data.frame(mtry=2)
modFit
#check Accuracy
confusionMatrix(predict (modFit, newdata = testingPC), testingPC$classe)
source('~/.active-rstudio-document', echo=TRUE)
10147/60
10147/60/60
stopCluster(cl) # Explicitly free up cores again.
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
View(training[,-notUsedColumns])
sum(is.na(training[,-notUsedColumns])))
sum(is.na(training[,-notUsedColumns]))
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
system.time ({
modFit <- train (classe ~ . ,
data=training[,-notUsedColumns], method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
#     modFit <- train (classe ~ . ,
#                  data=trainingPC, method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                   )
})
system.time ({
modFit <- train (classe ~ . ,
data=training[,-notUsedColumns], method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
#     modFit <- train (classe ~ . ,
#                  data=trainingPC, method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                   )
})
system.time ({
modFit <- train (classe ~ . ,
data=training[,-notUsedColumns], method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
#     modFit <- train (classe ~ . ,
#                  data=trainingPC, method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                   )
})
system.time ({
nPredictors <- dim (training[,-notUsedColumns])[2]-1
modFit <- train (classe ~ . ,
data=training[,-notUsedColumns], method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
#nPredictors <- dim (trainingPC)[2]-1
#     modFit <- train (classe ~ . ,
#                  data=trainingPC, method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                   )
})
str(training[,-notUsedColumns])
train <- fread(trainFile, sep = ",", header = TRUE, na.strings = c(""),
stringsAsFactors = FALSE, data.table = FALSE)
test <- fread(testFile, sep = ",", header = TRUE, na.strings = c(""),
stringsAsFactors = FALSE, data.table = FALSE)
names(train)[1] <- "id"
train$classe <- as.factor(train$classe)
train$new_window <- as.factor(train$new_window)
train$user_name <- as.factor(train$user_name)
train[train == "#DIV/0!"] <- NA
notUsedColumns <- c(1,5) #id, date - won't use as predictors
charColumns <- which(sapply(training, class) == "character")
charColumns <- setdiff(charColumns, notUsedColumns )
train[,charColumns] <- apply(train[, charColumns], 2, as.numeric)
str(train)
set.seed (1234)
inTrain <- createDataPartition(y = train$classe, p=0.01, list=FALSE)
training <- train [inTrain,]
testing <- train [-inTrain,]
str(training)
notUsedColumns
notUsedColumns <- union(notUsedColumns,
nearZeroVar (training, saveMetrics=FALSE, uniqueCut = 0.05))
notUsedColumns
names(training)[notUsedColumns]
numColumns <- setdiff(which(sapply(training, class) %in% c("numeric", "integer")), notUsedColumns)
preObj <- preProcess(training[,numColumns], method="knnImpute") #K-nearest neighbours imputation
training[,numColumns] <- predict(preObj, training[,numColumns])
numColumns
preObj
preObj <- preProcess(training[,numColumns], method="knnImpute") #K-nearest neighbours imputation
preObj
dim (traininÐ¿)
dim (training)
training[,numColumns] <- predict(preObj, training[,numColumns])
inTrain <- createDataPartition(y = train$classe, p=0.05, list=FALSE)
training <- train [inTrain,]
testing <- train [-inTrain,]
notUsedColumns <- union(notUsedColumns,
nearZeroVar (training, saveMetrics=FALSE, uniqueCut = 0.05))
#notUsedColumns
#names(training)[notUsedColumns]
#impute NA's for integer and numeric (not Factor or character)
numColumns <- setdiff(which(sapply(training, class) %in% c("numeric", "integer")), notUsedColumns)
preObj <- preProcess(training[,numColumns], method="knnImpute") #K-nearest neighbours imputation
training[,numColumns] <- predict(preObj, training[,numColumns])
preProc <- preProcess(training[,numColumns], method="pca", thresh=0.99)
preProc #35 columns (25%)
trainingPC <- predict(preProc, training[,numColumns])
#return Factor and outcome columns into the training set
trainingPC <- cbind(trainingPC,
training[,c("user_name", "new_window", "classe")])
#View(trainingPC)
testing[,numColumns] <- predict(preObj, testing[,numColumns])
testingPC <- predict (preProc, testing[,numColumns]) #get PC for testing based on the training preprocessing
#return Factor and outcome columns into the testing set
testingPC <- cbind(testingPC,
testing[,c("user_name", "new_window", "classe")])
str(training[,-notUsedColumns])
system.time ({
nPredictors <- dim (training[,-notUsedColumns])[2]-1
modFit <- train (classe ~ . ,
data=training[,-notUsedColumns], method = "rf", prox=TRUE,
trControl = trainControl (method="cv", number = 10, repeats = 10),
tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
ceiling(nPredictors/3),
ceiling(nPredictors/2),
ceiling(nPredictors*2/3),
nPredictors))
)
#nPredictors <- dim (trainingPC)[2]-1
#     modFit <- train (classe ~ . ,
#                  data=trainingPC, method = "rf", prox=TRUE,
#                  trControl = trainControl (method="cv", number = 10, repeats = 10),
#                  tuneGrid=data.frame(mtry=c(ceiling(sqrt(nPredictors)),
#                                             ceiling(nPredictors/3),
#                                             ceiling(nPredictors/2),
#                                             ceiling(nPredictors*2/3),
#                                             nPredictors))
#                   )
})
modFit
confusionMatrix(predict (modFit, newdata = testing), testing$classe)
stopCluster(cl) # Explicitly free up cores again.
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
